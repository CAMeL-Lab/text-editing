{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "# from tokenizers import Tokenizer\n",
    "from tokenizer import Tokenizer\n",
    "import json\n",
    "from tokenizers import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    with open(path) as f:\n",
    "        return [x.strip() for x in f.readlines()]\n",
    "    \n",
    "\n",
    "def read_data_json(path):\n",
    "    with open(path) as f:\n",
    "        return [json.loads(line) for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_tokenizer(tokenizer, execlude_len=5, path='tokenizer-5'):\n",
    "    words_to_remove = []\n",
    "    for word in tokenizer.vocab:\n",
    "        if word not in tokenizer.special_tokens_map.values() and len(word.replace('##', '')) > execlude_len:\n",
    "            words_to_remove.append(word)\n",
    "\n",
    "    print(f'{len(words_to_remove)} have been removed')\n",
    "    \n",
    "    model_state = json.loads(tokenizer.backend_tokenizer.model.__getstate__())\n",
    "\n",
    "    for word in words_to_remove:\n",
    "        del model_state['vocab'][word]\n",
    "    \n",
    "    model_class = getattr(models, model_state.pop(\"type\"))\n",
    "\n",
    "    tokenizer.backend_tokenizer.model = model_class(**model_state)\n",
    "\n",
    "    # tokenizer.save_pretrained(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_tokenizer_unk(tokenizer, execlude_len=5, path='tokenizer-5'):\n",
    "    words_to_remove = []\n",
    "    for word in tokenizer.vocab:\n",
    "        if word not in tokenizer.special_tokens_map.values() and len(word.replace('##', '')) > execlude_len:\n",
    "            words_to_remove.append(word)\n",
    "\n",
    "    print(f'{len(words_to_remove)} have been removed')\n",
    "    unk_token_id = tokenizer.convert_tokens_to_ids(\"[UNK]\")\n",
    "    for word in words_to_remove:\n",
    "        # Map the token to `[UNK]`'s ID\n",
    "        token_id = tokenizer.vocab[word]\n",
    "        tokenizer.ids_to_tokens[token_id] = \"[UNK]\"\n",
    "        tokenizer.vocab[word] = unk_token_id\n",
    "\n",
    "    # tokenizer.save_pretrained(path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "camel_tokenizer = AutoTokenizer.from_pretrained('CAMeL-Lab/bert-base-arabic-camelbert-msa')\n",
    "# resize_tokenizer_unk(camel_tokenizer)\n",
    "# resize_tokenizer(camel_tokenizer, execlude_len=4, path='tokenizer-4')\n",
    "# resize_tokenizer(camel_tokenizer, execlude_len=4, path='tokenizer-4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = Tokenizer('tokenizer-5')\n",
    "tokenizer = Tokenizer('CAMeL-Lab/bert-base-arabic-camelbert-msa')\n",
    "tokenizer_5 = Tokenizer('tokenizer-5')\n",
    "tokenizer_4 = Tokenizer('tokenizer-4')\n",
    "tokenizer_3 = Tokenizer('tokenizer-3')\n",
    "qalb14_dev = read_data_json(f'../arabic-gec/data/gec/modeling/qalb14/wo_camelira/full/dev.json')\n",
    "tokenized_dev = [tokenizer.tokenize(sent['raw'], flatten=True) for sent in qalb14_dev]\n",
    "tokenized_dev_5 = [tokenizer_5.tokenize(sent['raw'], flatten=True) for sent in qalb14_dev]\n",
    "tokenized_dev_4 = [tokenizer_4.tokenize(sent['raw'], flatten=True) for sent in qalb14_dev]\n",
    "tokenized_dev_3 = [tokenizer_3.tokenize(sent['raw'], flatten=True) for sent in qalb14_dev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(max(tokenized_dev, key=len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(max(tokenized_dev_5, key=len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(max(tokenized_dev_4, key=len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(max(tokenized_dev_3, key=len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = qalb14_dev[0]['raw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2, 2575, 1953, 13448, 3157, 10914, 2054, 1961, 13489, 2134, 5003, 1017, 2590, 4449, 1016, 2135, 1971, 378, 15524, 1906, 8047, 3866, 27527, 2075, 20126, 4872, 2155, 1908, 4034, 5660, 2059, 13983, 3827, 1956, 15707, 378, 2349, 8017, 21507, 2085, 2554, 21589, 28181, 8393, 20126, 10738, 2058, 13174, 378, 2038, 4133, 3766, 1006, 2790, 1011, 1912, 6030, 8920, 2134, 3629, 2249, 378, 6841, 2534, 2270, 5552, 7542, 2013, 2677, 9290, 1922, 13489, 18463, 22254, 13418, 2503, 2204, 1015, 26563, 1949, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer._tokenizer(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2, 2575, 1953, 14420, 1015, 3157, 10914, 2054, 1961, 6668, 1999, 2134, 5003, 1017, 2590, 4449, 1016, 2135, 1971, 378, 15524, 1906, 8047, 3866, 27527, 2075, 20126, 4872, 2155, 1908, 4034, 5660, 2059, 13983, 3827, 1956, 15707, 378, 2349, 8017, 21507, 2085, 2017, 2184, 21589, 28181, 8393, 20126, 10738, 2058, 2216, 4617, 378, 2038, 4133, 22989, 1911, 2790, 1011, 1912, 6030, 8920, 2134, 3629, 2249, 378, 6841, 2534, 2270, 5552, 7542, 2013, 2677, 9290, 1922, 6668, 1999, 18463, 22254, 13418, 2503, 2204, 1015, 26563, 1949, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_5._tokenizer(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
