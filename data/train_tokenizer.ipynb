{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00303089-5a61-418c-afd6-65819711c3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.pre_tokenizers import WhitespaceSplit\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers import Regex\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "78f68bfa-1414-4ddd-ba06-28df0d66f80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_edits(path):\n",
    "    with open(path) as f:\n",
    "        return [x.strip() for x in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "17f9de62-23ff-489d-9a20-d5fd46b4dbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniq_op(edits):\n",
    "    all_single_edits = []\n",
    "    for edit in edits:\n",
    "        # single_edits = re.findall(r'I_\\[.*?\\]+|A_\\[.*?\\]+|R_\\[.*?\\]+|K\\*|.', edit)\n",
    "        single_edits = re.findall(r'I_\\[.*?\\]+|A_\\[.*?\\]+|R_\\[.*?\\]+|K\\*|.', edit)\n",
    "        all_single_edits.extend([e for e in single_edits if e != ' '])\n",
    "    return Counter(all_single_edits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "ab7ce9d0-00e1-4e5c-b3a1-3f0e031055a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "edits = read_edits('qalb14_train_edits.seqs.txt')\n",
    "single_edits = uniq_op(edits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "1a6c56c1-1061-465d-b18f-c4ad83ec0cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('atomic_edits.txt', 'w') as f:\n",
    "    for k, v in single_edits.items():\n",
    "        f.write(f'{k}\\t{v}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "cc21aee7-5d8b-482e-b5dc-665be7e1f21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = WhitespaceSplit()\n",
    "trainer = WordPieceTrainer(special_tokens=single_edits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "925224cb-d959-4c98-a290-b9fd324d7882",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = ['qalb14_train_edits.seqs.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "2e93bb4b-9951-4a41-8c4a-3a226e9b5b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train(train_file, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "6f64b91a-8ec0-43ce-87ad-b741131ff725",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"tokenizer_edits.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "eeb73d25-eb3a-436c-83e9-2b0f927182bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenzier = Tokenizer.from_file('tokenizer_edits.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b76f51cd-ba72-474c-9d46-4096ee96b4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bla = tokenizer.encode('I_[يري]KKKKKKKKKKR_[و]R_[ن]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "820873a2-8436-4045-908f-a2c5f7418328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I_[يري]', 'KKKKKK', '##KK', '##KK', 'R_[و]', 'R_[ن]']"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bla.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a6893984",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oov(examples, tokenizer):\n",
    "    oov = set()\n",
    "    for example in examples:\n",
    "        words = example.split()\n",
    "        for word in words:\n",
    "            tokenized_word = tokenizer.tokenize(word, flatten='True')\n",
    "            if '[UNK]' in tokenized_word:\n",
    "                oov.add(word)\n",
    "\n",
    "    return oov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "28ab9f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/scratch/ba63/arabic-text-editing/data/coda/madar/train.preproc.raw.txt') as f:\n",
    "    examples = [line.strip() for line in f.readlines() if line.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "337501fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizer import Tokenizer\n",
    "tokenizer = Tokenizer('/scratch/ba63/BERT_models/bert-large-arabertv02')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "80489498",
   "metadata": {},
   "outputs": [],
   "source": [
    "oov = get_oov(examples, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6ef40cce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'بيچر؟'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8d2d2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
