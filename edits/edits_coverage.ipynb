{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    inputs, edits, sents = [], [], []\n",
    "    with open(path) as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                line = line.split('\\t')\n",
    "                inputs.append(line[0].replace('<s>', ''))\n",
    "                edits.append(line[1].replace('<s>', ''))\n",
    "            else:\n",
    "                sents.append({'sent': inputs, 'edits': edits})\n",
    "                inputs = []\n",
    "                edits = []\n",
    "        \n",
    "    if inputs or edits:\n",
    "        sents.append({'sent': inputs, 'edits': edits})\n",
    "    \n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_splits(data_dir, dataset='qalb14'):\n",
    "    train_word_no_cmpr = read_data(f'{data_dir}/edits_no_compressed/{dataset}/word-level/train_edits.modeling.tsv')\n",
    "    train_word_cmpr = read_data(f'{data_dir}/edits_compressed/{dataset}/word-level/train_edits.modeling.tsv')\n",
    "\n",
    "    dev_word_no_cmpr = read_data(f'{data_dir}/edits_no_compressed/{dataset}/word-level/dev_edits.modeling.tsv')\n",
    "    dev_word_cmpr = read_data(f'{data_dir}/edits_compressed/{dataset}/word-level/dev_edits.modeling.tsv')\n",
    "\n",
    "    train_subword_no_cmpr = read_data(f'{data_dir}/edits_no_compressed/{dataset}/subword-level/train_edits.modeling.tsv')\n",
    "    train_subword_cmpr = read_data(f'{data_dir}/edits_compressed/{dataset}/subword-level/train_edits.modeling.tsv')\n",
    "\n",
    "    dev_subword_no_cmpr = read_data(f'{data_dir}/edits_no_compressed/{dataset}/subword-level/dev_edits.modeling.tsv')\n",
    "    dev_subword_cmpr = read_data(f'{data_dir}/edits_compressed/{dataset}/subword-level/dev_edits.modeling.tsv')\n",
    "\n",
    "    return {'compressed':\n",
    "                {'word': {'train': train_word_cmpr, 'dev': dev_word_cmpr},\n",
    "                 'subword': {'train': train_subword_cmpr, 'dev': dev_subword_cmpr}\n",
    "                },\n",
    "            'no-compressed':\n",
    "                {'word': {'train': train_word_no_cmpr, 'dev': dev_word_no_cmpr},\n",
    "                 'subword': {'train': train_subword_no_cmpr, 'dev': dev_subword_no_cmpr}\n",
    "                }\n",
    "           }\n",
    "\n",
    "\n",
    "def read_splits_prune(data_dir, dataset='qalb14'):\n",
    "    train_subword_prune_10 = read_data(f'{data_dir}/edits_compressed_prune_10/{dataset}/subword-level/train_edits.modeling.tsv')\n",
    "    train_subword_prune_20 = read_data(f'{data_dir}/edits_compressed_prune_20/{dataset}/subword-level/train_edits.modeling.tsv')\n",
    "    train_subword_prune_30 = read_data(f'{data_dir}/edits_compressed_prune_30/{dataset}/subword-level/train_edits.modeling.tsv')\n",
    "\n",
    "    dev_subword_cmpr = read_data(f'{data_dir}/edits_compressed/{dataset}/subword-level/dev_edits.modeling.tsv')\n",
    "\n",
    "    return {'prune_10': {'train': train_subword_prune_10, 'dev': dev_subword_cmpr },\n",
    "            'prune_20': {'train': train_subword_prune_20, 'dev': dev_subword_cmpr},\n",
    "            'prune_30': {'train': train_subword_prune_30, 'dev': dev_subword_cmpr}\n",
    "            }\n",
    "\n",
    "\n",
    "\n",
    "def read_splits_prune_pnx_sep(data_dir, dataset='qalb14'):\n",
    "    train_subword_nopnx = read_data(f'{data_dir}/edits_compressed_pnx_sep/{dataset}/subword-level/train_edits_nopnx_edits.modeling.tsv')\n",
    "    train_subword_pnx = read_data(f'{data_dir}/edits_compressed_pnx_sep/{dataset}/subword-level/train_edits_pnx_edits.modeling.tsv')\n",
    "\n",
    "\n",
    "    train_subword_prune_nopnx_10 = read_data(f'{data_dir}/edits_compressed_pnx_sep_prune_10/{dataset}/subword-level/train_edits_nopnx_edits.modeling.tsv')\n",
    "    train_subword_prune_nopnx_20 = read_data(f'{data_dir}/edits_compressed_pnx_sep_prune_20/{dataset}/subword-level/train_edits_nopnx_edits.modeling.tsv')\n",
    "    train_subword_prune_nopnx_30 = read_data(f'{data_dir}/edits_compressed_pnx_sep_prune_30/{dataset}/subword-level/train_edits_nopnx_edits.modeling.tsv')\n",
    "\n",
    "\n",
    "    train_subword_prune_pnx_10 = read_data(f'{data_dir}/edits_compressed_pnx_sep_prune_10/{dataset}/subword-level/train_edits_pnx_edits.modeling.tsv')\n",
    "    train_subword_prune_pnx_20 = read_data(f'{data_dir}/edits_compressed_pnx_sep_prune_20/{dataset}/subword-level/train_edits_pnx_edits.modeling.tsv')\n",
    "    train_subword_prune_pnx_30 = read_data(f'{data_dir}/edits_compressed_pnx_sep_prune_30/{dataset}/subword-level/train_edits_pnx_edits.modeling.tsv')\n",
    "\n",
    "\n",
    "    dev_subword_cmpr_nopnx = read_data(f'{data_dir}/edits_compressed_pnx_sep/{dataset}/subword-level/dev_edits_nopnx_edits.modeling.tsv')\n",
    "    dev_subword_cmpr_pnx = read_data(f'{data_dir}/edits_compressed_pnx_sep/{dataset}/subword-level/dev_edits_pnx_edits.modeling.tsv')\n",
    "\n",
    "    return {'nopnx_prune_0': {'train': train_subword_nopnx, 'dev': dev_subword_cmpr_nopnx},\n",
    "            'nopnx_prune_10': {'train': train_subword_prune_nopnx_10, 'dev': dev_subword_cmpr_nopnx},\n",
    "            'nopnx_prune_20': {'train': train_subword_prune_nopnx_20, 'dev': dev_subword_cmpr_nopnx},\n",
    "            'nopnx_prune_30': {'train': train_subword_prune_nopnx_30, 'dev': dev_subword_cmpr_nopnx},\n",
    "\n",
    "            'pnx_prune_0': {'train': train_subword_pnx, 'dev': dev_subword_cmpr_pnx},\n",
    "            'pnx_prune_10': {'train': train_subword_prune_pnx_10, 'dev': dev_subword_cmpr_pnx},\n",
    "            'pnx_prune_20': {'train': train_subword_prune_pnx_20, 'dev': dev_subword_cmpr_pnx},\n",
    "            'pnx_prune_30': {'train': train_subword_prune_pnx_30, 'dev': dev_subword_cmpr_pnx},\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_stats_all(data, paper_print=False):\n",
    "    # Total Number of Edits\n",
    "    # Total Number of Uniq Edits\n",
    "    # % of Errors\n",
    "    # OOV rate\n",
    "    # if paper_print == False:\n",
    "    #     print(f'Split\\tGran.\\tComp.\\tPrune\\tTotal Edits\\tUnique Edits\\tUnique Errors\\tToken OOVs\\tType OOVs')\n",
    "    # else:\n",
    "    #     print(f'Gran.\\tComp.\\tSubset\\tPrune\\tUnique Edits\\tToken OOVs')\n",
    "    \n",
    "    for compr in ['no-compressed', 'compressed']:\n",
    "        for gran in ['word', 'subword']:\n",
    "            oov_stats = get_oov_rate(train_edits=[edit for example in data[compr][gran]['train']\n",
    "                                                  for edit in example['edits']],\n",
    "                                               test_edits=[edit for example in data[compr][gran]['dev']\n",
    "                                                           for edit in example['edits']])\n",
    "\n",
    "            for split in ['train', 'dev']:\n",
    "                edits = [edit for example in data[compr][gran][split]\n",
    "                                for edit in example['edits']]\n",
    "                edits_cnt = Counter(edits)\n",
    "\n",
    "                # total number of edits\n",
    "                num_edits = len(edits)\n",
    "\n",
    "                # total number of uniq edits\n",
    "                edits_uniq = len(edits_cnt)\n",
    "\n",
    "                # total number of errors\n",
    "                errors = sum([v for k, v in edits_cnt.items() if (k != 'K*' and set(k) != {'K'})])\n",
    "                no_errors = sum([v for k, v in edits_cnt.items() if (k == 'K*' or set(k) == {'K'})])\n",
    "\n",
    "                assert errors + no_errors == sum(edits_cnt.values())\n",
    "\n",
    "                if paper_print == False:\n",
    "                    if split == 'dev':\n",
    "                        print(f'{split}\\t{gran}\\t{True if compr == \"compressed\" else False}\\t{None}\\t{num_edits}\\t{edits_uniq}'\n",
    "                            f'\\t{errors}\\t{oov_stats[0]} ({oov_stats[1]:.2f}%)\\t{oov_stats[2]} ({oov_stats[3]:.2f}%)')\n",
    "                    else:\n",
    "                        print(f'{split}\\t{gran}\\t{True if compr == \"compressed\" else False}\\t{None}\\t{num_edits}\\t{edits_uniq}'\n",
    "                            f'\\t{errors}')\n",
    "\n",
    "                else:\n",
    "                    if split == 'dev':\n",
    "                        print(f'{gran}\\t{True if compr == \"compressed\" else False}\\tAll\\t{None}\\t{unique_train_edits}\\t{oov_stats[1]:.2f}%')\n",
    "                    else:\n",
    "                        unique_train_edits = edits_uniq\n",
    "\n",
    "\n",
    "def data_stats_prune(data, paper_print=False):\n",
    "    # Total Number of Edits\n",
    "    # Total Number of Uniq Edits\n",
    "    # % of Errors\n",
    "    # OOV rate\n",
    "    # if paper_print == False:\n",
    "        # print(f'Split\\tGran.\\tComp.\\tPrune\\tTotal Edits\\tUnique Edits\\tUnique Errors\\tToken OOVs\\tType OOVs')\n",
    "    # else:\n",
    "        # print(f'Gran.\\tComp.\\tSubset\\tPrune\\tUnique Edits\\tToken OOVs')\n",
    "    \n",
    "    for k in [10, 20, 30]:\n",
    "        oov_stats = get_oov_rate(train_edits=[edit for example in data[f'prune_{k}']['train']\n",
    "                                                for edit in example['edits']],\n",
    "                                            test_edits=[edit for example in data[f'prune_{k}']['dev']\n",
    "                                                        for edit in example['edits']])\n",
    "\n",
    "        for split in ['train', 'dev']:\n",
    "            edits = [edit for example in data[f'prune_{k}'][split]\n",
    "                            for edit in example['edits']]\n",
    "            edits_cnt = Counter(edits)\n",
    "\n",
    "            # total number of edits\n",
    "            num_edits = len(edits)\n",
    "\n",
    "            # total number of uniq edits\n",
    "            edits_uniq = len(edits_cnt)\n",
    "\n",
    "            # total number of errors\n",
    "            errors = sum([v for k, v in edits_cnt.items() if (k != 'K*' and set(k) != {'K'})])\n",
    "            no_errors = sum([v for k, v in edits_cnt.items() if (k == 'K*' or set(k) == {'K'})])\n",
    "\n",
    "            assert errors + no_errors == sum(edits_cnt.values())\n",
    "\n",
    "            if paper_print == False:\n",
    "                if split == 'dev':\n",
    "                    print(f'{split}\\tsubword\\tTrue\\t{k}\\t{num_edits}\\t{edits_uniq}'\n",
    "                            f'\\t{errors}\\t{oov_stats[0]} ({oov_stats[1]:.2f}%)\\t{oov_stats[2]} ({oov_stats[3]:.2f}%)')\n",
    "                else:\n",
    "                    print(f'{split}\\tsubword\\tTrue\\t{k}\\t{num_edits}\\t{edits_uniq}'\n",
    "                            f'\\t{errors}')\n",
    "            else:\n",
    "                if split == 'dev':\n",
    "                    print(f'subword\\tTrue\\tAll\\t{k}\\t{unique_train_edits}\\t{oov_stats[1]:.2f}%')\n",
    "                else:\n",
    "                    unique_train_edits = edits_uniq\n",
    "\n",
    "\n",
    "\n",
    "def data_stats_prune_pnx_sep(data, paper_print=False):\n",
    "    # Total Number of Edits\n",
    "    # Total Number of Uniq Edits\n",
    "    # % of Errors\n",
    "    # OOV rate\n",
    "    # if paper_print == False:\n",
    "    #     print(f'Pnx?\\tSplit\\tGran.\\tComp.\\tPrune\\tTotal Edits\\tUnique Edits\\tUnique Errors\\tToken OOVs\\tType OOVs')\n",
    "    # else:\n",
    "    #     print(f'Gran.\\tComp.\\tSubset\\tPrune\\tUnique Edits\\tToken OOVs')\n",
    "\n",
    "    for k in [0, 10, 20, 30]:\n",
    "        for exp in ['nopnx', 'pnx']:\n",
    "            oov_stats = get_oov_rate(train_edits=[edit for example in data[f'{exp}_prune_{k}']['train']\n",
    "                                                    for edit in example['edits']],\n",
    "                                                test_edits=[edit for example in data[f'{exp}_prune_{k}']['dev']\n",
    "                                                            for edit in example['edits']])\n",
    "\n",
    "            for split in ['train', 'dev']:\n",
    "                edits = [edit for example in data[f'{exp}_prune_{k}'][split]\n",
    "                                for edit in example['edits']]\n",
    "                edits_cnt = Counter(edits)\n",
    "\n",
    "                # total number of edits\n",
    "                num_edits = len(edits)\n",
    "\n",
    "                # total number of uniq edits\n",
    "                edits_uniq = len(edits_cnt)\n",
    "\n",
    "                # total number of errors\n",
    "                errors = sum([v for k, v in edits_cnt.items() if (k != 'K*' and set(k) != {'K'})])\n",
    "                no_errors = sum([v for k, v in edits_cnt.items() if (k == 'K*' or set(k) == {'K'})])\n",
    "\n",
    "                assert errors + no_errors == sum(edits_cnt.values())\n",
    "\n",
    "                if paper_print == False:\n",
    "                    if split == 'dev':\n",
    "                        print(f'{exp}\\t{split}\\tsubword\\tTrue\\t{k}\\t{num_edits}\\t{edits_uniq}'\n",
    "                                f'\\t{errors}\\t{oov_stats[0]} ({oov_stats[1]:.2f}%)\\t{oov_stats[2]} ({oov_stats[3]:.2f}%)')\n",
    "                    else:\n",
    "                        print(f'{exp}\\t{split}\\tsubword\\tTrue\\t{k}\\t{num_edits}\\t{edits_uniq}'\n",
    "                                f'\\t{errors}')\n",
    "                else:\n",
    "                    if split == 'dev':\n",
    "                        print(f'subword\\tTrue\\t{exp}\\t{k}\\t{unique_train_edits}\\t{oov_stats[1]:.2f}%')\n",
    "                    else:\n",
    "                        unique_train_edits = edits_uniq\n",
    "\n",
    "\n",
    "def get_oov_rate(train_edits, test_edits):\n",
    "    oov = 0\n",
    "    for edit in test_edits:\n",
    "        if edit not in train_edits:\n",
    "            oov += 1\n",
    "    oov_percentage = (oov / len(test_edits)) * 100\n",
    "\n",
    "    type_oov = 0\n",
    "    test_edits_cnts = Counter(test_edits)\n",
    "    train_edits_cnts = Counter(train_edits)\n",
    "\n",
    "    for edit in test_edits_cnts:\n",
    "        if edit not in train_edits_cnts:\n",
    "            type_oov += 1\n",
    "\n",
    "    type_oov_percentage = (type_oov / len(test_edits_cnts)) * 100\n",
    "\n",
    "    return oov, oov_percentage, type_oov, type_oov_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/msa-gec/edits/qalb14'\n",
    "qalb14 = read_splits(data_dir=data_dir, dataset='qalb14-arabertv02')\n",
    "qalb14_prune = read_splits_prune(data_dir=data_dir, dataset='qalb14-arabertv02')\n",
    "qalb14_pnx_sep = read_splits_prune_pnx_sep(data_dir=data_dir, dataset='qalb14-arabertv02')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gran.\tComp.\tSubset\tPrune\tUnique Edits\tToken OOVs\n",
      "word\tFalse\tAll\tNone\t16221\t1.00%\n",
      "subword\tFalse\tAll\tNone\t9060\t0.36%\n",
      "word\tTrue\tAll\tNone\t10410\t1.00%\n",
      "subword\tTrue\tAll\tNone\t6170\t0.36%\n",
      "\n",
      "subword\tTrue\tAll\t10\t683\t0.75%\n",
      "subword\tTrue\tAll\t20\t442\t1.02%\n",
      "subword\tTrue\tAll\t30\t329\t1.24%\n",
      "\n",
      "subword\tTrue\tnopnx\t0\t4799\t0.27%\n",
      "subword\tTrue\tpnx\t0\t160\t0.01%\n",
      "subword\tTrue\tnopnx\t10\t520\t0.56%\n",
      "subword\tTrue\tpnx\t10\t48\t0.02%\n",
      "subword\tTrue\tnopnx\t20\t335\t0.75%\n",
      "subword\tTrue\tpnx\t20\t35\t0.05%\n",
      "subword\tTrue\tnopnx\t30\t250\t0.92%\n",
      "subword\tTrue\tpnx\t30\t29\t0.05%\n"
     ]
    }
   ],
   "source": [
    "print(f'Gran.\\tComp.\\tSubset\\tPrune\\tUnique Edits\\tToken OOVs')\n",
    "\n",
    "data_stats_all(qalb14, paper_print=True)\n",
    "print()\n",
    "data_stats_prune(qalb14_prune, paper_print=True)\n",
    "print()\n",
    "data_stats_prune_pnx_sep(qalb14_pnx_sep, paper_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/msa-gec/edits/zaebuc'\n",
    "zaebuc = read_splits(data_dir=data_dir, dataset='zaebuc-arabertv02')\n",
    "zaebuc_prune = read_splits_prune(data_dir=data_dir, dataset='zaebuc-arabertv02')\n",
    "zaebuc_pnx_sep = read_splits_prune_pnx_sep(data_dir=data_dir, dataset='zaebuc-arabertv02')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gran.\tComp.\tSubset\tPrune\tUnique Edits\tToken OOVs\n",
      "word\tFalse\tAll\tNone\t1097\t2.94%\n",
      "subword\tFalse\tAll\tNone\t905\t1.85%\n",
      "word\tTrue\tAll\tNone\t687\t2.94%\n",
      "subword\tTrue\tAll\tNone\t563\t1.85%\n",
      "\n",
      "subword\tTrue\tAll\t10\t58\t3.71%\n",
      "subword\tTrue\tAll\t20\t35\t4.67%\n",
      "subword\tTrue\tAll\t30\t27\t5.26%\n",
      "\n",
      "subword\tTrue\tnopnx\t0\t498\t1.74%\n",
      "subword\tTrue\tpnx\t0\t23\t0.06%\n",
      "subword\tTrue\tnopnx\t10\t52\t3.39%\n",
      "subword\tTrue\tpnx\t10\t6\t0.11%\n",
      "subword\tTrue\tnopnx\t20\t30\t4.31%\n",
      "subword\tTrue\tpnx\t20\t6\t0.11%\n",
      "subword\tTrue\tnopnx\t30\t22\t4.90%\n",
      "subword\tTrue\tpnx\t30\t6\t0.11%\n"
     ]
    }
   ],
   "source": [
    "print(f'Gran.\\tComp.\\tSubset\\tPrune\\tUnique Edits\\tToken OOVs')\n",
    "data_stats_all(zaebuc, paper_print=True)\n",
    "print()\n",
    "data_stats_prune(zaebuc_prune, paper_print=True)\n",
    "print()\n",
    "data_stats_prune_pnx_sep(zaebuc_pnx_sep, paper_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/da-gec/edits/madar'\n",
    "madar = read_splits(data_dir=data_dir, dataset='madar-arabertv02')\n",
    "madar_prune = read_splits_prune(data_dir=data_dir, dataset='madar-arabertv02')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gran.\tComp.\tSubset\tPrune\tUnique Edits\tToken OOVs\n",
      "word\tFalse\tAll\tNone\t1228\t1.52%\n",
      "subword\tFalse\tAll\tNone\t677\t0.55%\n",
      "word\tTrue\tAll\tNone\t741\t1.52%\n",
      "subword\tTrue\tAll\tNone\t454\t0.55%\n",
      "\n",
      "subword\tTrue\tAll\t10\t84\t1.33%\n",
      "subword\tTrue\tAll\t20\t52\t2.02%\n",
      "subword\tTrue\tAll\t30\t45\t2.28%\n"
     ]
    }
   ],
   "source": [
    "print(f'Gran.\\tComp.\\tSubset\\tPrune\\tUnique Edits\\tToken OOVs')\n",
    "data_stats_all(madar, paper_print=True)\n",
    "print()\n",
    "data_stats_prune(madar_prune, paper_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simplification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
